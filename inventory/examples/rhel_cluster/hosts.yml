---
# Example inventory for RHEL+Slurm cluster deployment
#
# This inventory defines a complete HPC cluster with:
# - Login nodes
# - CPU compute nodes
# - GPU compute nodes (A100)
# - Storage nodes
#
# Usage:
#   ansible-playbook -i inventory/examples/rhel_cluster/hosts.yml playbooks/deploy_rhel_cluster.yml

all:
  children:
    bcm_headnode:
      hosts:
        bcm-headnode:
          ansible_host: 192.168.122.204
          ansible_user: root
          # Using SSH key authentication

      vars:
        # ================================================================
        # Cluster Configuration
        # ================================================================
        cluster_name: hpc01
        cluster_type: slurm

        # ================================================================
        # BCM Network Configuration
        # ================================================================
        # Internal provisioning network IP (nodes access repos via this IP)
        bcm_internal_ip: "10.141.255.254"

        # ================================================================
        # RHEL Configuration
        # ================================================================
        rhel_version: "9.6"
        rhel_iso_path: /root/rhel-9.6-x86_64-dvd.iso

        # Root password for installed systems (CHANGE THIS!)
        root_password: "changeme123"

        # ================================================================
        # Network Configuration
        # ================================================================
        network_bootproto: dhcp
        network_device: link
        network_onboot: true

        # ================================================================
        # Disk Configuration
        # ================================================================
        disk_device: vda  # VMs use virtio disks (vda), physical servers use sda
        disk_clearpart: true
        disk_autopart_type: lvm

        # ================================================================
        # Node Roles
        # ================================================================
        node_roles:
          # Login nodes
          - name: login
            category: "slurm-{{ cluster_name }}-login"
            packages:
              - "@^minimal-environment"
              - gcc
              - gcc-c++
              - make
              - git
              - nfs-utils
              - vim-enhanced
              - tmux
            post_script: |
              # Configure NFS mounts
              echo "{{ bcm_internal_ip }}:/home /home nfs defaults 0 0" >> /etc/fstab
              echo "{{ bcm_internal_ip }}:/cm/shared /shared nfs defaults 0 0" >> /etc/fstab
              # Mount will happen on first boot

          # CPU Compute nodes
          - name: compute-cpu
            category: "slurm-{{ cluster_name }}-compute-cpu"
            packages:
              - "@^minimal-environment"
              - gcc
              - gcc-c++
              - make
              - git
              - nfs-utils
              - vim-enhanced
            post_script: |
              # Configure NFS mounts
              echo "{{ bcm_internal_ip }}:/home /home nfs defaults 0 0" >> /etc/fstab
              echo "{{ bcm_internal_ip }}:/cm/shared /shared nfs defaults 0 0" >> /etc/fstab
              # Disable unnecessary services
              systemctl disable firewalld

          # GPU Compute nodes (A100)
          - name: compute-gpu-a100
            category: "slurm-{{ cluster_name }}-compute-gpu-a100"
            packages:
              - "@^minimal-environment"
              - gcc
              - gcc-c++
              - make
              - git
              - nfs-utils
              - kernel-devel
              - kernel-headers
              - pciutils
              - vim-enhanced
            post_script: |
              # Configure NFS mounts
              echo "{{ bcm_internal_ip }}:/home /home nfs defaults 0 0" >> /etc/fstab
              echo "{{ bcm_internal_ip }}:/cm/shared /shared nfs defaults 0 0" >> /etc/fstab
              # Blacklist nouveau driver (for NVIDIA driver installation)
              echo "blacklist nouveau" > /etc/modprobe.d/blacklist-nouveau.conf
              echo "options nouveau modeset=0" >> /etc/modprobe.d/blacklist-nouveau.conf
              # NVIDIA driver installation will be done post-deployment

          # Storage nodes
          - name: storage
            category: "slurm-{{ cluster_name }}-storage"
            packages:
              - "@^minimal-environment"
              - "@file-server"
              - nfs-utils
              - lvm2
            post_script: |
              # NFS server configuration will be done post-deployment
              echo "Storage node configured"

        # ================================================================
        # Cluster Nodes (2 test VMs)
        # ================================================================
        cluster_nodes:
          # CPU Compute nodes
          - name: compute-cpu-01
            mac: "52:54:00:20:01:01"
            ip: "10.141.120.1"
            category: "slurm-{{ cluster_name }}-compute-cpu"

          - name: compute-cpu-02
            mac: "52:54:00:20:01:02"
            ip: "10.141.120.2"
            category: "slurm-{{ cluster_name }}-compute-cpu"
